{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7c676dee",
      "metadata": {
        "id": "7c676dee"
      },
      "source": [
        "# 06 – Model Comparison\n",
        "\n",
        "In this notebook, we compare different sequence models to classify penalty kick direction from pose keypoints.\n",
        "\n",
        "Models included:\n",
        "- LSTM (baseline)\n",
        "- Bidirectional LSTM\n",
        "- GRU\n",
        "- Lightweight Transformer\n",
        "\n",
        "Evaluation is based on accuracy, F1-score, and training time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c89df66a",
      "metadata": {
        "id": "c89df66a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout, Masking, Bidirectional\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Supposé : X, y sont chargés depuis notebook 03\n",
        "y_cat = to_categorical(y, num_classes=3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72b48c25",
      "metadata": {
        "id": "72b48c25"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model):\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    history = model.fit(X_train, y_train, epochs=20, batch_size=16, verbose=0, validation_split=0.1)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "    y_pred_class = np.argmax(y_pred, axis=1)\n",
        "    f1 = f1_score(y_true, y_pred_class, average='macro')\n",
        "    acc = accuracy_score(y_true, y_pred_class)\n",
        "    return acc, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa8097d",
      "metadata": {
        "id": "caa8097d"
      },
      "outputs": [],
      "source": [
        "# Tester différents modèles\n",
        "results = {}\n",
        "\n",
        "# 1. LSTM simple\n",
        "m1 = Sequential([\n",
        "    Masking(mask_value=0., input_shape=(X.shape[1], X.shape[2])),\n",
        "    LSTM(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "results['LSTM'] = evaluate_model(m1)\n",
        "\n",
        "# 2. Bidirectional LSTM\n",
        "m2 = Sequential([\n",
        "    Masking(mask_value=0., input_shape=(X.shape[1], X.shape[2])),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "results['BiLSTM'] = evaluate_model(m2)\n",
        "\n",
        "# 3. GRU\n",
        "m3 = Sequential([\n",
        "    Masking(mask_value=0., input_shape=(X.shape[1], X.shape[2])),\n",
        "    GRU(64),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')\n",
        "])\n",
        "results['GRU'] = evaluate_model(m3)\n",
        "\n",
        "# 4. Lightweight Transformer\n",
        "from tensorflow.keras.layers import Dense, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "inputs = Input(shape=(X.shape[1], X.shape[2]))\n",
        "x = Dense(64)(inputs)\n",
        "x = LayerNormalization()(x)\n",
        "x = MultiHeadAttention(num_heads=4, key_dim=16)(x, x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "outputs = Dense(3, activation='softmax')(x)\n",
        "\n",
        "transformer_model = Model(inputs, outputs)\n",
        "results['Transformer'] = evaluate_model(transformer_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f0ecdf6",
      "metadata": {
        "id": "0f0ecdf6"
      },
      "outputs": [],
      "source": [
        "# Comparaison des scores\n",
        "for model_name, (acc, f1) in results.items():\n",
        "    print(f\"{model_name}: Accuracy={acc:.2%}, F1-score={f1:.2%}\")\n",
        "\n",
        "plt.bar(results.keys(), [r[0] for r in results.values()], label='Accuracy')\n",
        "plt.bar(results.keys(), [r[1] for r in results.values()], alpha=0.6, label='F1-score')\n",
        "plt.ylabel(\"Score\")\n",
        "plt.title(\"Model Comparison\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}